# 香橙派AI Pro综合开发笔记 - 3

# 8. OLLAMA本地AI模型服务部署

## 概述

Ollama是一个用于在本地运行、管理和部署大型语言模型（LLM）的开源工具。它通过容器化部署，将模型的下载、加载和推理服务封装成简单的API，使得在边缘设备（如香橙派AI Pro）上运行LLM成为可能。本部分将详细阐述在CasaOS系统中部署Ollama的完整流程、配置细节、以及排查各类问题的系统化方法。

## 详细部署流程

### 1. 前期准备与环境确认

在开始部署前，请通过SSH连接至香橙派AI Pro，并执行以下命令以确认系统状态：

```bash
# 确认Docker服务状态
sudo systemctl status docker
# 确认可用内存资源（关键，直接影响模型选择）
free -h
# 确认CasaOS数据存储路径
ls -la /DATA/
```

理想的基线是Docker服务处于活动状态，且系统拥有至少4GB的可用内存。

### 2. CasaOS自定义安装配置

由于CasaOS应用商店可能未预置Ollama，需使用“自定义安装”功能。点击应用商店右上角的“+”或“Custom Install”，填写以下关键参数：

* **Docker Image**: `ollama/ollama:latest`
* **容器端口 (Container Port)**: `11434`
* **Web UI端口 (Host Port)**: 选择一个未被占用的端口，例如`11435`。
* **卷映射 (Volumes)**: 这是保证数据持久化的关键配置。
  * **主机 (Host)**: `/DATA/AppData/ollama` （建议路径，便于集中管理）
  * **容器 (Container)**: `/root/.ollama`
* **设备 (Devices)**: 保持为空或按需配置。早期版本尝试挂载`/dev`目录可能导致`cgroup`错误，如无特殊需求建议留空。
* **环境变量**: 初始部署通常无需额外环境变量。

### 3. 部署后初始化与模型下载

应用启动后，需要通过容器终端执行命令来下载模型。

1. **进入容器终端**：在CasaOS应用界面，点击已部署的Ollama应用图标，通常会有“终端”或“Console”入口。或通过SSH使用命令：
   ```bash
   docker exec -it <容器名称或ID> /bin/bash
   ```
2. **下载模型**：在容器终端内，执行下载命令。**请务必根据设备内存选择微型模型**：
   ```bash
   # 选项1: 超小型模型，约0.5B参数
   ollama pull qwen:0.5b
   # 选项2: 小型模型，约1.1B参数
   ollama pull tinyllama:1.1b
   # 选项3: 小型对话模型，约3B参数（需内存充足）
   ollama pull neural-chat:3b
   ```
3. **验证模型运行**：下载完成后，可进行简易测试。
   ```bash
   # 启动交互式对话（按Ctrl+D退出）
   ollama run qwen:0.5b
   # 或通过API进行一次性测试
   curl http://127.0.0.1:11434/api/generate -d '{"model": "qwen:0.5b", "prompt": "你好", "stream": false}'
   ```

## 常见故障与调试路径汇总

在部署和运行过程中，你可能会遇到以下问题，以下是经过验证的解决路径。

### 问题1：容器启动失败，报错 `invalid cgroup device type “p”`

* **问题现象**：在CasaOS中启动Ollama应用失败，日志或报错信息中包含 `invalid cgroup device type “p”`。
* **原因分析**：此错误通常与Docker容器的“设备(Devices)”配置中填写了无效或格式不完整的设备路径有关（例如一个孤立的“p”字符）。也可能是Docker守护进程的cgroup状态异常。
* **解决路径**：
  1. **检查配置**：进入Ollama应用的编辑页面，检查“设备(Devices)”配置项。**确保该字段内容完整、正确，或直接清空该字段**。
  2. **重启Docker服务**：在SSH中执行 `sudo systemctl restart docker`。此操作会短暂影响所有容器，但能重置Docker的cgroup状态。
  3. **重启宿主系统**：如果上述步骤无效，重启香橙派是最彻底的解决方案。

### 问题2：容器名称冲突导致部署失败

* **问题现象**：重复安装或重启应用时，报错 `Conflict. The container name “...“ is already in use ...`。
* **原因分析**：CasaOS或Docker尝试创建一个与现有容器同名的新容器。
* **解决路径**：
  1. 通过SSH执行命令，列出所有相关容器并确认其状态：
     ```bash
     docker ps -a --filter “name=ollama”
     ```
  2. 强制删除处于 `Created`、`Exited` 或 `Dead` 状态的残留容器：
     ```bash
     docker rm -f <残留容器的ID>
     ```
  3. 返回CasaOS，删除旧的应用实例，然后重新进行“自定义安装”。

### 问题3：模型下载进度条长时间卡住（例如75%）

* **问题现象**：使用 `ollama pull` 命令时，下载速度降至0，进度条长时间停滞。
* **原因分析**：这通常是**正常现象**。Ollama下载分为两步：第一步是从网络下载模型压缩包（显示下载速度），第二步是在本地解压、校验和转换模型格式（速度显示为0，进度条暂停）。第二步完全依赖CPU和磁盘I/O，在香橙派上可能耗时较长。
* **解决路径**：
  1. **切勿中断**：不要关闭终端或停止容器。
  2. **验证活动**：打开另一个SSH终端，通过以下命令观察系统是否在处理：
     ```bash
     # 查看Ollama容器日志
     docker logs -f <ollama容器名>
     # 观察系统资源占用
     htop
     ```
  3. 只要CPU或磁盘I/O有较高占用，或日志显示 `verifying sha256 digest`、`writing manifest` 等信息，请耐心等待10-30分钟或更久。

### 问题4：运行模型时提示 `Error: 500 Internal Server Error: llama runner process has terminated: signal: killed`

* **问题现象**：执行 `ollama run` 命令后，模型加载过程中进程被终止。
* **原因分析**：这是典型的**内存不足（OOM）** 问题。系统内核因内存耗尽而强制终止了模型进程。
* **解决路径**：
  1. **检查容器内存限制**（最常见原因）：

     ```bash
     docker inspect <ollama容器ID> | grep -i memory
     ```

     如果输出显示 `“Memory”: 268435456`（即256MB），则限制过低。
  2. **调整内存限制**：在CasaOS中编辑Ollama应用设置，找到内存限制选项，将其修改为 **`4096M` (4GB)** 或 **`8192M` (8GB)**，保存并重启应用。
  3. **选择更小模型**：如果调整限制后问题依旧，表明物理内存紧张，必须换用更小的模型（如从 `llama3.2:1b` 换为 `qwen:0.5b`）。
  4. **启用系统交换空间**：作为辅助手段，为系统创建交换文件可以提供虚拟内存缓冲。

     ```bash
     sudo fallocate -l 2G /swapfile
     sudo chmod 600 /swapfile
     sudo mkswap /swapfile
     sudo swapon /swapfile
     # 使用 free -h 确认Swap已启用
     ```

### 问题5：API请求或WebUI无法连接到Ollama服务

* **问题现象**：浏览器访问 `http://<香橙派IP>:11434` 显示拒绝连接，或Open WebUI无法获取模型列表。
* **原因分析**：端口映射错误、容器未运行或防火墙阻止。
* **解决路径**：
  1. **确认容器状态与端口映射**：

     ```bash
     docker ps | grep ollama  # 确认状态为Up
     # 确认端口映射，应看到 0.0.0.0:11434->11434/tcp 或类似
     ```
  2. **从宿主机内部测试**：

     ```bash
     curl http://127.0.0.1:11434/api/tags
     ```

     如果宿主机能通但外部不通，检查CasaOS中Ollama应用的“Web UI端口”映射设置是否正确。
  3. **检查防火墙**：如果校园网或系统防火墙阻止了 `11434` 端口，需配置放行规则。

# 9. Open WebUI可视化前端部署与集成

## 概述

Open WebUI（原Ollama WebUI）是一个功能丰富的Web应用程序，为Ollama提供了一个类似于ChatGPT的友好图形界面。它通过Docker容器独立部署，并通过网络API与后端的Ollama服务通信。成功集成后，用户可通过浏览器进行模型对话、管理聊天历史等操作。

## 详细部署流程

### 1. 通过CasaOS自定义安装部署Open WebUI

与部署Ollama类似，使用“自定义安装”功能。

* **Docker Image**: `ghcr.io/open-webui/open-webui:main` (可考虑使用稳定版本标签如 `:0.8.5`)
* **容器端口 (Container Port)**: `8080`
* **Web UI端口 (Host Port)**: 选择一个未被占用的新端口，例如 `11436`。
* **卷映射 (Volumes)**:
  * **主机 (Host)**: `/DATA/AppData/open_webui` （用于持久化聊天数据、用户配置）
  * **容器 (Container)**: `/app/backend/data`
* **环境变量 (Environment)**：**这是最关键的一步，必须正确配置。**
  * **键 (Key)**: `OLLAMA_BASE_URL`
  * **值 (Value)**: 指向你正在运行的Ollama服务地址。有两种填写方式：
    1. **使用宿主机IP**: `http://192.168.31.213:11434` （请替换为你的实际IP）
    2. **使用Docker容器名（推荐，更稳定）**: `http://musing_fredrik-main_app-1:11434`
* **重启策略 (Restart Policy)**: 建议设置为 `always` 或 `unless-stopped`。

### 2. 部署后初始化与使用

1. **访问界面**：安装完成后，在浏览器访问 `http://<香橙派IP>:11436`。
2. **首次注册**：首次打开会提示创建管理员账户，完成注册后登录。
3. **连接模型后端**：登录后，Open WebUI应能自动通过配置的 `OLLAMA_BASE_URL` 发现Ollama后端。你可以在设置中检查连接状态。
4. **拉取与选择模型**：在WebUI的模型管理页面，你可以直接拉取新模型（会调用后端Ollama的API），或直接选择已在Ollama中下载好的模型开始对话。

## 常见故障与调试路径汇总

### 问题1：部署后无法通过浏览器访问Open WebUI

* **问题现象**：应用显示为运行中，但无法打开Web页面。
* **原因分析**：端口映射错误、容器启动失败、或健康检查未通过。
* **解决路径**：
  1. **确认端口与状态**：

     ```bash
     docker ps | grep open-webui
     ```

     确认状态为 `Up`，并查看正确的宿主机映射端口（如 `0.0.0.0:11436->8080/tcp`）。
  2. **检查容器日志**：日志能直接反映启动失败原因。

     ```bash
     docker logs --tail 100 <open-webui容器名>
     ```
  3. **常见日志错误与解决**：

     * **无限循环打印初始化日志**：通常因无法连接Ollama后端导致。重点检查 `OLLAMA_BASE_URL` 环境变量。
     * **权限错误 (Permission denied)**：数据卷目录权限不足。执行：
       ```bash
       sudo chown -R 1000:1000 /DATA/AppData/open_webui
       ```

### 问题2：Open WebUI中“选择模型”处为空，无法添加模型

* **问题现象**：WebUI界面可以打开，但模型列表为空，或无法连接到后端。
* **原因分析**：Open WebUI未能成功与Ollama服务通信。
* **解决路径**：
  1. **进入容器内部进行连接测试**（最有效的诊断）：
     ```bash
     docker exec -it <open-webui容器名> bash
     # 在容器内执行：
     echo $OLLAMA_BASE_URL  # 确认环境变量值
     curl -v $OLLAMA_BASE_URL/api/tags  # 测试网络连通性
     ```
  2. **根据测试结果处理**：
     * **`echo` 输出为空**：环境变量未设置。需在CasaOS中编辑应用，添加 `OLLAMA_BASE_URL`。
     * **`curl` 返回连接失败**：网络不通。将 `OLLAMA_BASE_URL` 的值改为使用Ollama的**容器名**（如 `http://musing_fredrik-main_app-1:11434`），这通常在Docker网络内更可靠。
     * **`curl` 返回成功但模型列表为空**：连接正常，但Ollama中未下载模型。需返回Ollama容器下载模型。

### 问题3：容器不断重启，状态为 `Restarting`

* **问题现象**：在CasaOS中应用状态异常，容器日志显示启动后很快退出。
* **原因分析**：启动命令执行完毕即退出，或遇到致命错误。
* **解决路径**：
  1. 检查日志，通常末尾会有错误信息。
  2. **指定启动命令**：在CasaOS编辑页面，尝试在“命令”或“CMD”字段中填入 `bash start.sh`，强制容器运行启动脚本。
  3. **更换镜像版本**：将 `:main` 标签换为具体的稳定版本，如 `:0.8.5`。
  4. **清理数据卷重新安装**：有时旧数据会导致初始化失败。可以删除应用，并清空主机上的 `/DATA/AppData/open_webui` 目录，然后重新安装。

## 最佳实践与最终建议

1. **部署顺序**：务必先确保Ollama服务部署成功并能运行模型，再部署Open WebUI。
2. **网络配置**：在Docker Compose或CasaOS管理的环境下，**使用容器名而非IP地址**来配置服务间连接（如 `OLLAMA_BASE_URL`），这能避免因IP变动导致连接失败。
3. **资源监控**：持续在CasaOS系统状态页或通过 `htop` 命令监控内存使用情况。同时运行Ollama和Open WebUI对内存有一定要求。
4. **模型管理**：始终从超小型模型开始测试。模型的参数量（B）是影响内存占用的主要因素，而非单纯的磁盘大小。
5. **数据持久化**：务必为Ollama和Open WebUI正确配置卷映射，防止模型文件和聊天记录在容器更新或重建时丢失。

通过遵循以上详细的步骤和调试指南，你应能在香橙派AI Pro上成功构建一个运行在校园网内的、私有的、带可视化界面的本地聊天机器人应用。

[上一篇](2.md)
[下一篇](4.md)